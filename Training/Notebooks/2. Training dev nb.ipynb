{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Training dev nb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VimalMollyn/ED6001-Term-Project/blob/colab/Training/Notebooks/2.%20Training%20dev%20nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv1VVaDjK1ec",
        "outputId": "69617f73-2937-4697-9687-6d8ad00cc71c"
      },
      "source": [
        "## stuff to include at the start of each notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone -b colab https://github.com/VimalMollyn/ED6001-Term-Project.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'ED6001-Term-Project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1mjV0WckFbY",
        "outputId": "d922638d-66c6-4a3d-c756-712dffbb6d8b"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.7)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makasharidas\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ausM4SOo-kck"
      },
      "source": [
        "# !rm -r /content/ED6001-Term-Project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9QqrGmL7vP"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'ED6001-Term-Project/Training/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hov55xbhLfyY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "351879bd-8f6c-41be-942c-6a3047bd9b07"
      },
      "source": [
        "## imports go here\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity, normalized_root_mse\n",
        "import wandb\n",
        "wandb.init(project=\"mia_final\", entity=\"akasharidas\")\n",
        "\n",
        "from preprocessing import patch_test_img, merge_test_img\n",
        "# from data import MRIDataset ## imports for the MRI dataset\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class GeneratorNet(nn.Module):\n",
        "    \"\"\" \n",
        "    In the original implementation, post-activation values were being added to pre-activation values \n",
        "    Modified it to add pre-activation values, then apply activation\n",
        "    Removed final ReLU\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(GeneratorNet, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv3d(1, 32, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(32),)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.LeakyReLU(inplace=True), nn.Conv3d(32, 64, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(64),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.LeakyReLU(inplace=True), nn.Conv3d(64, 128, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(128),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.LeakyReLU(inplace=True), nn.Conv3d(128, 256, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(256),\n",
        "        )\n",
        "\n",
        "        self.deConv1 = nn.Sequential(\n",
        "            nn.LeakyReLU(inplace=True), nn.Conv3d(256, 128, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(128),\n",
        "        )\n",
        "\n",
        "        self.deConv2 = nn.Sequential(\n",
        "            nn.LeakyReLU(inplace=True), nn.Conv3d(128, 64, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(64),\n",
        "        )\n",
        "\n",
        "        self.deConv3 = nn.Sequential(\n",
        "            nn.LeakyReLU(inplace=True), nn.Conv3d(64, 32, kernel_size=3, padding=1, bias=True), nn.BatchNorm3d(32),\n",
        "        )\n",
        "\n",
        "        self.deConv4 = nn.Sequential(nn.LeakyReLU(inplace=True), nn.Conv3d(32, 1, kernel_size=3, padding=1),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        conv3 = self.conv3(conv2)\n",
        "        conv4 = self.conv4(conv3)\n",
        "\n",
        "        out = self.deConv1(conv4)\n",
        "        out += conv3\n",
        "\n",
        "        out = self.deConv2(out)\n",
        "        out += conv2\n",
        "\n",
        "        out = self.deConv3(out)\n",
        "        out += conv1\n",
        "\n",
        "        out = self.deConv4(out)\n",
        "        out += x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DiscriminatorNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorNet, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv3d(1, 32, kernel_size=3, padding=1), nn.LeakyReLU(),)\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv3d(32, 64, kernel_size=3, padding=1), nn.LeakyReLU(),)\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv3d(64, 128, kernel_size=3, padding=1), nn.LeakyReLU(),)\n",
        "\n",
        "        self.fc = nn.Linear(128 * 6 * 32 * 32, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for i in size:\n",
        "            num_features *= i\n",
        "\n",
        "        return num_features\n",
        "\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG19, self).__init__()\n",
        "        vgg19 = models.vgg19(pretrained=True)\n",
        "        self.feature = vgg19.features\n",
        "    '''\n",
        "    input: N*1*D(6)*H*W\n",
        "    output: N*C*H*W\n",
        "    '''\n",
        "\n",
        "    def forward(self, input):\n",
        "        # VGG19: means:103.939, 116.779, 123.68\n",
        "        input /= 16\n",
        "        depth = input.size()[2]\n",
        "        result = []\n",
        "        for i in range(depth):\n",
        "            x = torch.cat(\n",
        "                (input[:, :, i, :, :] - 103.939, input[:, :, i, :, :] - 116.779, input[:, :, i, :, :] - 123.68), 1)\n",
        "            result.append(self.feature(x))\n",
        "\n",
        "        output = torch.cat(result, dim=1)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makasharidas\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/akasharidas/mia_final/runs/2ta2wsa9\" target=\"_blank\">stellar-snowflake-1</a></strong> to <a href=\"https://wandb.ai/akasharidas/mia_final\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohdln8DRMoJW"
      },
      "source": [
        "def initialize_weights(*models):\n",
        "    for model in models:\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Conv3d) or isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.BatchNorm3d):\n",
        "                module.weight.data.fill_(1)\n",
        "                module.bias.data.zero_()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8BYcjNmOLO_"
      },
      "source": [
        "class WGAN():\n",
        "    def __init__(self, level=0):\n",
        "        # parameters\n",
        "        self.epochs = 10\n",
        "        self.batch_size = 110\n",
        "        self.lr =5e-6\n",
        "\n",
        "        self.d_iter = 5\n",
        "        self.lambda_gp = 10\n",
        "\n",
        "        self.lambda_vgg = 1e-1\n",
        "        self.lambda_d = 1e-3\n",
        "        self.lambda_mse = 1\n",
        "\n",
        "        self.level = level\n",
        "\n",
        "        self.loss_dir = \"./loss/\"\n",
        "        self.v = \"0_0_5_%d\" % self.level  # vs\n",
        "        self.save_dir = \"./model/\" + self.v + \"/\"\n",
        "        Path(self.loss_dir).mkdir(parents=True, exist_ok=True)\n",
        "        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n",
        "        self.gpu = False\n",
        "\n",
        "        self.generator = GeneratorNet()\n",
        "        self.discriminator = DiscriminatorNet()\n",
        "        self.vgg19 = VGG19()\n",
        "\n",
        "        self.G_optimizer = optim.Adam(\n",
        "            self.generator.parameters(), lr=self.lr, betas=(0.5, 0.9))\n",
        "        self.D_optimizer = optim.Adam(\n",
        "            self.discriminator.parameters(), lr=self.lr, betas=(0.5, 0.9))\n",
        "\n",
        "        self.G_loss = nn.MSELoss()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.generator.cuda()\n",
        "            self.discriminator.cuda()\n",
        "            self.vgg19.cuda()\n",
        "            self.gpu = True\n",
        "        if not self.load_model():\n",
        "            initialize_weights(self.generator)\n",
        "            initialize_weights(self.discriminator)\n",
        "\n",
        "    def train(self, trainloader, validloader):\n",
        "        self.dataloader = trainloader\n",
        "        self.validDataloader = validloader\n",
        "        examples_seen = 0\n",
        "\n",
        "        ## training loop\n",
        "        for epoch in range(0, self.epochs):\n",
        "\n",
        "            # iterate over the dataset\n",
        "            for batch_index, batch in enumerate(self.dataloader):\n",
        "                clean_img = batch[\"clean_img\"]\n",
        "                noised_img = batch[\"noisy_img\"]\n",
        "                examples_seen += clean_img.shape[0]\n",
        "\n",
        "                # train discriminator\n",
        "                for iter_i in range(self.d_iter):\n",
        "                    loss = self._train_discriminator(clean_img, noised_img)\n",
        "\n",
        "                wandb.log({\n",
        "                    \"discriminator_loss\": loss[0],\n",
        "                    \"neg_real_validity\": loss[1],\n",
        "                    \"fake_validity\": loss[2],\n",
        "                    \"gradient_penalty\": loss[3],\n",
        "                    \"examples_seen\": examples_seen\n",
        "                })\n",
        "\n",
        "                # train generator\n",
        "                loss = self._train_generator(clean_img, noised_img)\n",
        "\n",
        "                wandb.log({\n",
        "                    \"generator_loss\": loss[0],\n",
        "                    \"mse_loss\": loss[1],\n",
        "                    \"neg_fake_validity\": loss[2],\n",
        "                    \"perceptual_loss\": loss[3],\n",
        "                    \"examples_seen\": examples_seen\n",
        "                })\n",
        "\n",
        "\n",
        "                # save model and loss\n",
        "                if batch_index % 100 == 0:\n",
        "                    self.save_model()\n",
        "\n",
        "\n",
        "            if ((epoch + 1) % 4 == 0 and self.lr > 1e-7):\n",
        "                self.G_optimizer.defaults[\"lr\"] *= 0.5\n",
        "                self.G_optimizer.defaults[\"lr\"] *= 0.5\n",
        "                self.lr *= 0.5\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.test(examples_seen)\n",
        "\n",
        "\n",
        "    def _train_discriminator(self, clean_img, noised_img, train=True):\n",
        "        self.D_optimizer.zero_grad()\n",
        "\n",
        "        z = Variable(noised_img)\n",
        "        real_img = Variable(clean_img / 4096)\n",
        "        if self.gpu:\n",
        "            z = z.cuda()\n",
        "            real_img = real_img.cuda()\n",
        "\n",
        "        fake_img = self.generator(z)\n",
        "        real_validity = self.discriminator(real_img)\n",
        "        fake_validity = self.discriminator(fake_img.data / 4096)\n",
        "        gradient_penalty = self._calc_gradient_penalty(\n",
        "            real_img.data, fake_img.data)\n",
        "\n",
        "        d_loss = torch.mean(-real_validity) + torch.mean(fake_validity) + \\\n",
        "            self.lambda_gp * gradient_penalty\n",
        "        if train:\n",
        "            d_loss.backward()\n",
        "            self.D_optimizer.step()\n",
        "\n",
        "        return d_loss.data.item(), torch.mean(-real_validity).cpu().item(), torch.mean(fake_validity).cpu().item(), self.lambda_gp * gradient_penalty.cpu().item()\n",
        "\n",
        "    def _train_generator(self, clean_img, noised_img, train=True):\n",
        "        z = Variable(noised_img)\n",
        "        real_img = Variable(clean_img, requires_grad=False)\n",
        "\n",
        "\n",
        "        if self.gpu:\n",
        "            z = z.cuda()\n",
        "            real_img = real_img.cuda()\n",
        "\n",
        "        self.G_optimizer.zero_grad()\n",
        "        self.D_optimizer.zero_grad()\n",
        "        self.vgg19.zero_grad()\n",
        "\n",
        "        criterion_mse = nn.MSELoss()\n",
        "        criterion_vgg= nn.MSELoss()\n",
        "\n",
        "        fake_img = self.generator(z)\n",
        "        mse_loss = criterion_mse(fake_img, real_img)\n",
        "        if train:\n",
        "            (self.lambda_mse * mse_loss).backward(retain_graph=True)\n",
        "\n",
        "\n",
        "        feature_fake_vgg = self.vgg19(fake_img)\n",
        "        feature_real_vgg = Variable(self.vgg19(real_img).data, requires_grad=False).cuda()\n",
        "\n",
        "        vgg_loss = criterion_vgg(feature_fake_vgg, feature_real_vgg)\n",
        "\n",
        "        fake_validity = self.discriminator(fake_img / 4096)\n",
        "        g_loss =  self.lambda_vgg * vgg_loss + self.lambda_d * torch.mean(-fake_validity)\n",
        "\n",
        "        if train:\n",
        "            g_loss.backward()\n",
        "            self.G_optimizer.step()\n",
        "        return g_loss.data.item(), mse_loss.data.item(), torch.mean(-fake_validity).data.item(), vgg_loss.data.item()\n",
        "\n",
        "    def _calc_gradient_penalty(self, clean_img, gen_img):\n",
        "        batch_size = clean_img.size()[0]\n",
        "        alpha = Variable(torch.rand(batch_size, 1))\n",
        "        alpha = alpha.expand(batch_size, clean_img.nelement(\n",
        "        ) // batch_size).contiguous().view(clean_img.size()).float()\n",
        "        if self.gpu:\n",
        "            alpha = alpha.cuda()\n",
        "\n",
        "        interpolates = (alpha * clean_img + (1 - alpha)\n",
        "                        * gen_img).requires_grad_(True)\n",
        "        disc_interpolates = self.discriminator(interpolates)\n",
        "        fake = Variable(torch.Tensor(batch_size, 1).fill_(1.0),\n",
        "                        requires_grad=False)\n",
        "        if self.gpu:\n",
        "            fake = fake.cuda()\n",
        "\n",
        "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                                  grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "        return gradient_penalty\n",
        "\n",
        "    def test(self, examples_seen):\n",
        "        self.generator.eval()\n",
        "        self.discriminator.eval()\n",
        "        \n",
        "        timestr = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "        print(timestr)\n",
        "\n",
        "        # test set\n",
        "        total_mse_loss = 0\n",
        "        total_g_loss = 0\n",
        "        total_d_loss = 0\n",
        "        total_vgg_loss = 0\n",
        "        batch_num = 0\n",
        "        for batch_index, batch in enumerate(self.validDataloader):\n",
        "            clean_img = batch[\"clean_img\"]\n",
        "            noisy_img = batch[\"noisy_img\"]\n",
        "\n",
        "            loss = self._train_generator(clean_img, noisy_img, train=False)\n",
        "            total_g_loss += loss[0]\n",
        "            total_mse_loss += loss[1]\n",
        "            total_d_loss += loss[2]\n",
        "            total_vgg_loss += loss[3]\n",
        "            batch_num += 1\n",
        "        mse_loss = total_mse_loss / batch_num\n",
        "        g_loss = total_g_loss / batch_num\n",
        "        d_loss = total_d_loss / batch_num\n",
        "        vgg_loss = total_vgg_loss / batch_num\n",
        "\n",
        "        wandb.log({\n",
        "            \"test_discriminator_loss\": d_loss,\n",
        "            \"test_generator_loss\": g_loss,\n",
        "            \"test_perceptual_loss\": vgg_loss,\n",
        "            \"test_mse_loss\": mse_loss,\n",
        "            \"examples_seen\": examples_seen\n",
        "            }\n",
        "        )\n",
        "        # self.compute_quality()\n",
        "        self.save_loss((vgg_loss, mse_loss, g_loss, d_loss))\n",
        "        self.save_model()\n",
        "\n",
        "        self.generator.train()\n",
        "        self.discriminator.train()\n",
        "\n",
        "    def compute_quality(self):\n",
        "        psnr1 = 0\n",
        "        psnr2 = 0\n",
        "        mse1 = 0\n",
        "        mse2 = 0\n",
        "        ssim1 = 0\n",
        "        ssim2 = 0\n",
        "        _psnr1 = 0\n",
        "        _psnr2 = 0\n",
        "        _mse1 = 0\n",
        "        _mse2 = 0\n",
        "        _ssim1 = 0\n",
        "        _ssim2 = 0\n",
        "\n",
        "        for i in range(101, 111):\n",
        "            free_nii = nib.load(\"./data/dataset/Free/%d.nii\" % i)\n",
        "            noised_nii = nib.load(\n",
        "                \"./data/dataset/noise_%d/%d.nii\" % (self.level, i))\n",
        "\n",
        "            clean_img = free_nii.get_data()[:, :144, :].astype(np.int16)\n",
        "            noisy_img = noised_nii.get_data()[:, :144, :].astype(np.int16)\n",
        "            patchs, row, col = patch_test_img(noisy_img)\n",
        "            denoisy_img = merge_test_img(\n",
        "                self.denoising(patchs), row, col).astype(np.int16)\n",
        "\n",
        "            psnr1 += peak_signal_noise_ratio(clean_img, noisy_img, 4096)\n",
        "            psnr2 += peak_signal_noise_ratio(clean_img, denoisy_img, 4096)\n",
        "\n",
        "            mse1 += normalized_root_ms(clean_img, noisy_img)\n",
        "            mse2 += normalized_root_ms(clean_img, denoisy_img)\n",
        "\n",
        "            ssim1 += structural_similarity(clean_img, noisy_img,\n",
        "                                  data_range=4096, multichannel=True)\n",
        "            ssim2 += structural_similarity(clean_img, denoisy_img,\n",
        "                                  data_range=4096, multichannel=True)\n",
        "\n",
        "            max = np.max(clean_img)\n",
        "            _psnr1 += peak_signal_noise_ratio(clean_img, noisy_img, max)\n",
        "            _psnr2 += peak_signal_noise_ratio(clean_img, denoisy_img, max)\n",
        "\n",
        "            _mse1 += normalized_root_ms(clean_img, noisy_img)\n",
        "            _mse2 += normalized_root_ms(clean_img, denoisy_img)\n",
        "\n",
        "            _ssim1 += structural_similarity(clean_img, noisy_img,\n",
        "                                   data_range=max, multichannel=True)\n",
        "            _ssim2 += structural_similarity(clean_img, denoisy_img,\n",
        "                                   data_range=max, multichannel=True)\n",
        "            \n",
        "        psnr1 *= 0.1\n",
        "        psnr2 *= 0.1\n",
        "        mse1 *= 0.1\n",
        "        mse2 *= 0.1\n",
        "        ssim1 *= 0.1\n",
        "        ssim2 *= 0.1\n",
        "\n",
        "        _psnr1 *= 0.1\n",
        "        _psnr2 *= 0.1\n",
        "        _mse1 *= 0.1\n",
        "        _mse2 *= 0.1\n",
        "        _ssim1 *= 0.1\n",
        "        _ssim2 *= 0.1\n",
        "        with open(\"./loss/\" + self.v + \"psnr.csv\", \"a+\") as f:\n",
        "            f.write(\"%f,%f,%f,%f,%f,%f\\n\" %\n",
        "                    (_psnr1, _psnr2, _ssim1, _ssim2, _mse1, _mse2))\n",
        "        timestr = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "        with open(\"./loss/\" + self.v + \"psnr_4096.csv\", \"a+\") as f:\n",
        "            f.write(\"%s: %.10f,%f,%f,%f,%f,%f,%f\\n\" %\n",
        "                    (timestr, self.lr, psnr1, psnr2, ssim1, ssim2, mse1, mse2))\n",
        "        print(\"psnr: %f,%f,ssim: %f,%f,mse:%f,%f\\n\" %\n",
        "              (_psnr1, _psnr2, _ssim1, _ssim2, _mse1, _mse2))\n",
        "\n",
        "    '''\n",
        "    N*H*W*D -> N*C*D*H*W\n",
        "    return: N*H*W*D\n",
        "    '''\n",
        "\n",
        "    def denoising(self, patchs):\n",
        "        n, h, w, d = patchs.shape\n",
        "        denoised_patchs = []\n",
        "        for i in range(0, n, self.batch_size):\n",
        "            batch = patchs[i:i + self.batch_size]\n",
        "            batch_size = batch.shape[0]\n",
        "            x = np.reshape(batch, (batch_size, 1, w, h, d))\n",
        "            x = x.transpose(0, 1, 4, 2, 3)\n",
        "            x = Variable(torch.from_numpy(x).float()).cuda()\n",
        "            y = self.generator(x)\n",
        "            denoised_patchs.append(y.cpu().data.numpy())\n",
        "\n",
        "        denoised_patchs = np.vstack(denoised_patchs)\n",
        "        denoised_patchs = np.reshape(denoised_patchs, (n, d, h, w))\n",
        "        denoised_patchs = denoised_patchs.transpose(0, 2, 3, 1)\n",
        "        return denoised_patchs\n",
        "\n",
        "    def save_model(self):\n",
        "        if not os.path.exists(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "        torch.save(self.generator.state_dict(),\n",
        "                   self.save_dir + \"G_\" + self.v + \".pkl\")\n",
        "        torch.save(self.discriminator.state_dict(),\n",
        "                   self.save_dir + \"D_\" + self.v + \".pkl\")\n",
        "\n",
        "    def load_model(self):\n",
        "        if os.path.exists(self.save_dir + \"G_\" + self.v + \".pkl\") and \\\n",
        "                os.path.exists(self.save_dir + \"D_\" + self.v + \".pkl\"):\n",
        "\n",
        "            self.generator.load_state_dict(\n",
        "                torch.load(self.save_dir + \"G_\" + self.v + \".pkl\")\n",
        "            )\n",
        "            self.discriminator.load_state_dict(\n",
        "                torch.load(self.save_dir + \"D_\" + self.v + \".pkl\")\n",
        "            )\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def save_loss(self, loss):\n",
        "        value = \"\"\n",
        "        for item in loss:\n",
        "            value = value + str(item) + \",\"\n",
        "        value += \"\\n\"\n",
        "        with open(\"./loss/\" + self.v + \".csv\", \"a+\") as f:\n",
        "            f.write(value)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sVTpfdIEtpe"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import numpy as np\n",
        "\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, path_to_data, split=\"train\"):\n",
        "        if split == \"train\":\n",
        "            split_indices = [*range(1, 101)]\n",
        "        elif split == \"test\":\n",
        "            split_indices = [*range(101, 111)]\n",
        "\n",
        "        print(f\"Start reading {split}ing dataset...\")\n",
        "\n",
        "        # read the first set of volumes\n",
        "        clean_mri_set = []\n",
        "        noisy_mri_set = []\n",
        "\n",
        "        for i in tqdm_notebook(split_indices):\n",
        "            # load the current volumes\n",
        "            clean_mri_temp = np.load(path_to_data / f\"data/{i}.npy\")\n",
        "            noisy_mri_temp = np.load(path_to_data / f\"noisy/{i}.npy\")\n",
        "\n",
        "            # append to the existing stack\n",
        "            clean_mri_set.append(clean_mri_temp)\n",
        "            noisy_mri_set.append(noisy_mri_temp)\n",
        "\n",
        "        self.clean_mri_set = clean_mri_set\n",
        "        self.noisy_mri_set = noisy_mri_set\n",
        "        self.arrshape0 = clean_mri_set[0].shape[0]\n",
        "        self.total = len(self.clean_mri_set) * self.arrshape0\n",
        "        self.current_patch = 1\n",
        "\n",
        "        print(len(self.clean_mri_set))\n",
        "        print(self.total)\n",
        "        print(f\"End reading {split}ing dataset...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = index // self.arrshape0\n",
        "        y = index % self.arrshape0\n",
        "        clean_img = torch.from_numpy(self.clean_mri_set[x][y]).float().squeeze(axis=1)\n",
        "        noisy_img = torch.from_numpy(self.noisy_mri_set[x][y]).float().squeeze(axis=1)\n",
        "        return {\"clean_img\": clean_img, \"noisy_img\": noisy_img}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "XxhAkmqoO0C9",
        "outputId": "c96a3968-f907-46d2-94fe-3b45e3183b12"
      },
      "source": [
        "path_to_data = Path(\"/content/drive/MyDrive/ED6001_MIA_Term_Project/patches\")\n",
        "wgan = WGAN()\n",
        "\n",
        "trainset = MRIDataset(path_to_data=path_to_data, split=\"train\")\n",
        "trainloader = DataLoader(trainset, batch_size=wgan.batch_size, shuffle=True)\n",
        "validset = MRIDataset(path_to_data=path_to_data, split=\"test\")\n",
        "validloader = DataLoader(validset, batch_size=wgan.batch_size, shuffle=True)\n",
        "\n",
        "wgan.train(trainloader, validloader)\n",
        "\n",
        "    # # valid\n",
        "    # for i in range(101, 111):\n",
        "\n",
        "    #     nii_img = nib.load(\"./data/dataset/noise_%d/%d.nii\" % (level, i))\n",
        "    #     x = nii_img.get_data()\n",
        "    #     patchs, row, col = patch_test_img(x)\n",
        "    #     print(patchs.shape)\n",
        "    #     denoised_img = merge_test_img(wgan.denoising(patchs), row, col)\n",
        "    #     print(denoised_img.shape)\n",
        "    #     denoised_img = denoised_img.astype(np.int16)\n",
        "\n",
        "    #     denoised_image = nib.Nifti1Image(\n",
        "    #         denoised_img, nii_img.affine, nii_img.header)\n",
        "    #     nib.save(denoised_image, \"./result/%d_wgan_vgg_mse_denoised_img%d.nii\" % (level, i))\n",
        "    # # print((x - denoised_img).mean())\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ac18dfe4a3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# # valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fae799b05e4c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainloader, validloader)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0miter_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoised_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 wandb.log({\n",
            "\u001b[0;32m<ipython-input-9-fae799b05e4c>\u001b[0m in \u001b[0;36m_train_discriminator\u001b[0;34m(self, clean_img, noised_img, train)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreal_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_gp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAHR059AR3G9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "b05cac30-9506-4a7a-bbc1-727ac8fc845a"
      },
      "source": [
        "# Clear GPU memory\n",
        "wgan = None\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "1/0;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6abec2edcb8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1mdT2ETROFv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}